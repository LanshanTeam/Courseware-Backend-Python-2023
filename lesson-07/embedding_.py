import torch
import torch.nn as nn

# 假设有一个词汇表大小为10，每个词嵌入维度为3
vocab_size = 10
embedding_dim = 3

# 创建词嵌入层
embedding = nn.Embedding(vocab_size, embedding_dim)

# 假设有一个包含两个样本的输入，每个样本有4个单词
input_words = torch.LongTensor([[1, 2, 4, 5], [4, 3, 2, 9]])

# 将输入单词索引传递给词嵌入层，得到词嵌入矩阵
word_embeddings = embedding(input_words)

# 打印词嵌入矩阵
print(word_embeddings)

"""
tensor([[[-0.9920,  0.7159, -1.0296],
         [ 0.5718, -0.1504,  0.4619],
         [-1.3106, -0.9321,  1.1739],
         [ 0.2480,  1.9137, -2.1597]],

        [[-1.3106, -0.9321,  1.1739],
         [-0.0344,  0.7106,  0.5809],
         [ 0.5718, -0.1504,  0.4619],
         [-1.5760,  0.9687,  0.3797]]], grad_fn=<EmbeddingBackward0>)
"""

print(embedding.weight)

"""
Parameter containing:
tensor([[ 0.5201, -0.6770, -1.3804],
        [ 2.6152, -1.3307,  0.2908],
        [-0.3248, -1.6289, -0.1328],
        [-0.1395,  0.5597,  1.1708],
        [ 1.3347,  1.7835, -0.8744],
        [ 2.2921,  0.1958, -0.0789],
        [-0.3342,  0.5067,  0.6638],
        [ 1.3804, -0.7038, -1.7502],
        [ 1.7495, -1.2663, -1.7533],
        [ 0.1371,  0.6841, -0.6419]], requires_grad=True)
"""
